\documentclass[a4paper,12pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{hyperref}
\usepackage{graphicx}

\title{%

Pr\'actica 2: Limpieza y an\'alisis de los datos
	
	\author{%
	Enrique Vilanova Vidal%
		\thanks{Todos los que han colaborado en el proyecto}
	\and Brais Suarez Souto
	}
}

\begin{document}

\maketitle
\thispagestyle{empty}
\clearpage
\pagenumbering{arabic} 
\newpage

\textbf{Notas previas al ejercicio:} \\

Hemos contestado a las preguntas de acuerdo a lo indicado al ejercicio, sin embargo debido a la extensi\'on del trabajo y del c\'odigo, \'unicamente algunas partes del c\'odigo y su desarrollo  se han incluido en este texto. El objetivo es facilitar la comprensi\'on del desarrollo de las diversas partes del trabajo. El c\'odigo en su totalidad se puede encontrar en el enlace que se facilita a  \href{https://github.com/b-suarez/youtube_stats_analysis}{GitHub}.

El c\'odigo se ha hecho sobre {\itshape Google Colab} para facilitar la reproducci\'on y ejecuci\'on del trabajo. La primera parte del c\'odigo est\'a implementado en {\itshape Python} y la segunda parte en {\itshape R}. Las dos partes se han implementado sobre el mismo notebook.

En {\itshape Python} se ha implementado la limpieza general de datos, el desarrollo de m\'etricas, transformaciones exponenciales de los datos, gr\'aficas de correlaci\'on y todo lo relacionado con {\itshape Natural Language Processing}.

En {\itshape R} se ha implementado modelos de regresi\'on, an\'alisis de componente principal y test estad\'isticos.

Con el fin de aumentar la reproducibilidad del  trabajo, en el repositorio se incluyen objetos pickle y archivos csv con los datasets intermedios.

\newpage
\section[item_descripcion]{Descripci\'on del dataset. ?`Por qu\'e es importante y qu\'e  pregunta/problema pretende responder?}

\subsection{Descripci\'on del dataset.}

El conjunto de datos objeto de an\'alisis se ha obtenido a partir de este enlace en Kaggle y esta constituido por 10 datasets (uno por cada mercado regional: Canada, Alemania, Francia, Gran Breta\~na, India, Japon, Korea, M\'ejico, Rusia, Estados Unidos) de 16 caracter\'isticas (columnas) que representan los videos que han llegado a ser Trending en sus respectivas regiones.

Las caracter\'isticas representadas son las siguientes:

\begin{itemize}

\item \textbf{video{\textunderscore}id}: ID unico para cada v\'ideo.
\item \textbf{trending{\textunderscore}date}: fecha en la que el v\'ideo entr\'o en Trend.
\item \textbf{title}: T\'itulo del v\'ideo.
\item \textbf{channel{\textunderscore}title}: T\'itulo del canal al que se ha subido el v\'ideo.
\item \textbf{category{\textunderscore}id}: ID de la categor\'ia del v\'ideo. (El nombre de cada categor\'ia viene dado en un fichero json externo)
\item \textbf{publish{\textunderscore}time}: D\'ia y hora a la que se ha subido el v\'ideo.
\item \textbf{tags}: SEO tags introducidas en el v\'ideo.
\item \textbf{view}: N\'umero de visualizaciones que ha tenido el v\'ideo.
\item \textbf{likes}: N\'umero de likes.
\item \textbf{dislikes}: N\'umero de dislikes.
\item \textbf{comment{\textunderscore}count}: N\'umero de comentarios.
\item \textbf{thumbnail{\textunderscore}link}: Link d\'onde se almacena la imagen de la thumbnail.
\item \textbf{comments{\textunderscore}disabled}: Booleano que define si se han desactivado los comentarios.
\item \textbf{ratings{\textunderscore}disabled}: Booleano que define si se han desactivado los ratings.
\item \textbf{video{\textunderscore}error{\textunderscore}or{\textunderscore}removed}: Booleano que define si se han el video se ha borrado por algun error o manualmente.
\item \textbf{description}: Descripci\'on del video.

\end{itemize}

\subsection[item_importancia]{?`Por qu\'e es importante y qu\'e  pregunta/problema pretende responder?}

El dataset se ha elegido como continuaci\'on o extensi\'on de los datos recopilados en la pr\'actica 1. A modo de recordatorio, en la pr\'actica anterior nuestro objetivo era reconocer los temas que resultaban de mayor inter\'es a la mayor\'ia de la poblaci\'on en un momento en particular. Para cumplir este objetivo nos concentramos en la extracci\'on de datos de las plataformas:  {\itshape Meneame}, {\itshape Reddit} y {\itshape Twitter}. Sin embargo otra plataforma muy popular es {\itshape Youtube} de la que podemos extraer similar informaci\'on, adem\'as los atributos que obtenemos de la plataforma {\itshape Youtube} son comparables a los que obtenemos de las tres plataformas que estudiamos en la primera pr\'actica y un procesado de datos muy similar se puede establecer para los cuatro plataformas.

A partir del conjunto de datos para  {\itshape Youtube}  se plantean diferentes preguntas y an\'alisis que pueden que pueden extrapolarse f\'acilmente a los datos presentados en la primera pr\'actica. En nuestro caso hemos intentado responder a varias preguntas:

\begin{itemize}

\item  ?` qu\'e categorias son mas populares en cada mercado?
\item ?` cu\'anto tarda un video en hacerse Trending?
\item ?`Cuales son las palabras m\'s utilizadas?
\item ?`Cuales son las tags m\'as relevantes?
\item ?`Cuales son las temas m\'as importantes?
\item ?`Existen diferencias en el vocabulario que se utilizan los pa\'ises que hablan la misma lengua?
\item ?`Cual es el sentimiento y la polaridad expresada en los t\'itulos?
\item ?`Podemos modelar en n\'umero de veces que un v\'ideo se ve y establecer un modelo predictivo?

\end{itemize}

Estos an\'alisis son de gran relevancia actualmente ya que como todos sabemos, la industria de creaci\'on de contenido online genera una gran cantidad de dinero y p\'ublico. Saber navegar \'este mercado puede ser muy interesante para cualquier marca o individuo que quiera generar beneficios o atraer p\'ublico a trav\'es de YouTube.

\section{Integraci\'on y selecci\'on de los datos de inter\'es a analizar}

Tal y como se menciona en la secci\'on anterior este conjunto de datos extra\'idos de {\itshape Kaggle} cuenta con 10 datasets correspondientes a datos para la plataforma {\itshape YouTube} para diez pa\'ises diferentes. Los datasets de los diferentes pa\'ises cuentan con los mismos atributos. En general, los datsets tambi\'en presentan un fallo en la codificaci\'on de los caracteres que hace que las columnas con texto cuyos caracteres no est\'an dentro del alfabeto ingl\'es, no se cargen correctamente. Debido a esto se han seleccionado 4 datasets a tendiendo a una serie de criterios:

\begin{itemize}

\item Podemos leer entender el idioma del pa\'is donde provienen los datos. De este modo, podemos corregir problemas de codificaci\'on de caracteres en un tiempo razonable.
\item Representan zonas geogr\'aficas de varios continentes, en caso de que queramos hacer comparaciones regionales. 
\item Al menos dos datsets deben provenir de una regiones diferentes con la misma lengua. Esto permite hacer comparaciones en el uso del lenguaje.



\end{itemize} 

Atendiendo a estos criterios hemos seleccionado 4 datsets sobre  los que aplicaremos los mismo algoritmos. Todos los datos ser\'an tratados de la misma forma. En caso de que alg\'un dataset requiera un tratamiento especial en alg\'un punto se especificar\'a el motivo.

Los 4 datasets contienen datos para Alemania, Reino Unido, Mexico y EEUU.

\subsection{Descarga de datos}

Puesto que los datos ya contienen error de codificaci\'on de los atributos de texto y queremos evitar cualquier posible corrupci\'on de los datos al pasar por una m\'aquina intermedia (nuestros ordenadores), vamos a realizar una conexi\'on con Kaggle a trav\'es de su API utilizando nuestro fichero kaggle.json d\'onde guardaremos nuestras claves personales y descargamos los datos directamente a nuestro {\itshape drive} de google desde donde cargaremos los datos a nuestro notebook de {\itshape Google Colab}. 

\begin{figure}[h!]
 \centering
\includegraphics[width=13cm]{kaggle_upload.png}
\caption{Carga de datos desde {\itshape Kaggle}}
\label{fig:carga}
\end{figure}


Una vez nos hemos descargado los datasets y los tenemos en nuestro cloud, vamos a descomprimir el fichero y cargar los distintos datasets en un array de datasets que contendr\'a los diferentes datasets de cada mercado.
\begin{figure}[h!]
\centering
\includegraphics[width=7cm]{data_load.png}
\caption{Descomprimiendo datos desde {\itshape Google Drive}}
\label{fig:desc}
\end{figure}

\section[item_limpieza]{Limpieza de datos}

\subsection{?`Los datos contienen ceros o elementos vac\'ios? ?`C\'omo gestionar\'ias cada uno de estos casos?}

Para nuestros casos no hay valores nulos, como se puede ver en la siguiente figura\ref{fig:nula} izquierda:

\begin{figure}[h!]
\centering
\includegraphics[width=6cm]{vacio_datos.JPG}
\includegraphics[width=6cm]{Na_values.JPG}
\caption{A la izquierda valores nulos. A la derecha valores {\itshape NA}}
\label{fig:nula}
\end{figure}

Los posibles elementos vac\'ios en los atributos de texto se tratan de forma separada durante la limpieza de texto.

Cuando buscamos para valores {\itshape NA} (figura \ref{fig:nula} derecha), vemos que los valores vac\'ios se encuentran en la columna {\itshape description} (atributo texto), de la que no extraemos informaci\'on en nuestro an\'alisis. A\'un as\'i, no ha habido motivo de tratar estos valores directamente ya que durante otros procesos de limpieza, las obsevaciones  que contienen {\itshape NAs} han sido eliminadas por otros motivos. Tambi\'en cabe mencionar que antes de empezar nuestro an\'alisis en {\itshape R} hemos eliminado expl\'icitamente todas las observaciones que conten\'ian valores {\itshape NA}.


\subsection{Errores en en los c\'odigos de tiempo}

Una de las primeras errores que hemos podido detectar es que los datos referentes a c\'odigos de tiempo se encontraban en formatos diferentes. Por lo que se ha unificado los formatos de tiempo y convertirlos datos de {\itshape object} a {\itshape datetime}.

\subsubsection{Correcci\'on  trending date}

Transformaci\'on de todas las columnas que contengan un valor de fecha a tipo datetime, empezando por la columna trending date.

\begin{figure}[h!]
 \centering
\includegraphics[width=13cm]{correc_trending.png}
\caption{Conversi\'on para {\itshape trending date}}
\label{fig:trend_date}
\end{figure}

\subsubsection{Correcci\'on  publish time}
Vamos a realizar la misma operaci\'on con la columna publish time.

\begin{figure}[h!]
\centering
\includegraphics[width=13cm]{correc_publish.png}
\caption{Conversi\'on para {\itshape publish time}}
\label{fig:pub_date}
\end{figure}


\subsection{Correcci\'on  textos}
La parte de {\itshape language cleaning} es muy importante para nuestro proyecto, ya que tener textos limpios y analizables nos ser\'a \'util para futuros   an\'alisis.


\subsubsection{Limpieza general}

En nuestro caso hemos definido diferentes funciones para corregir caracteres mal codificados, emoticonos, signos de puntuaci\'on que no necesitamos para el an\'alisis del lenguaje natura, as\'i como una funci\'on espec\'ifica para tratar carateres especiales del alfabeto alem\'an l. Hemos aplicado a las columnas: title, channel{\textunderscore}title,  tags y description. Las funciones se presentan el figura \ref{fig:corrtext}.



\begin{figure}[h!]
\centering
\includegraphics[width=10cm]{latin_characters.png}\\
\includegraphics[width=10cm]{german_characters.png}\\
\includegraphics[width=10cm]{misspellings.png}
\caption{Correci\'on de texto}
\label{fig:corrtext}
\end{figure}

A de m\'as de las correcciones presentadas han sido necesarias, varias rondas de correci\'on de texto donde se han eliminado errores que no se hab\'ian capturado inicialmente. Todo esto se puede ver dentro del c\'odigo en la secci\'on {\itshape Some NLP-More Cleaning}

\subsubsection{{\itshape Stop Words} para el {\itshape document-Term Matrix}}

Con el fin de realizar las {\itshape World Clouds}, hay que eliminar del {\itshape document-Term Matrix} todas las {\itshape Stop Words}. El m\'odulo {\itshape Count Vectorizer} de {\itshape sklearn} ya proporciona una lista de {\itshape Stop Words} para ingl\'es pero debe ampliarse para otros lenguajes. De proyectos anteriores, ya dispon\'iamos de listas de {\itshape Stop words} para otros idiomas, que hemos a\~nadido, como se puede ver en la figura \ref{fig:stop}.

\begin{figure}[h!]
\centering
\includegraphics[width=10cm]{Stop_words.JPG}\\
\caption{A\~nadiendo {\itshape Stop Words}}
\label{fig:stop}
\end{figure}

A de m\'as de estas {\itshape Stop Words} ha sido necesario a\~nadir manualmente m\'as palabras debido a los errores de codificaci\'on de los textos

\subsection{Identificaci\'on y tratamiento de valores extremos}

Para nuestro caso los valores extremos en visitas o cuentas de comentarios marcan los v\'ideos m\'as populares, por lo que no parece acertado tratar estos datos como si de errores se tratasen, ya que estos valores extremos nos pueden indicar casos interesantes de estudio. por otra parte, al estudiar cuanto tiempo tarda un v\'ideo en convertirse  en tendencia, hemos visto que hay v\'ideos que pueden tardar hasta once a\~nos en llegar a tendencia. Estos caso, si son outliers  para nosotros. Nuestro caso de estudio es, como las cosas que ocurrieron ayer afecta a los que ser\'a tendencia ma\~nana y cuanto dura el impacto de estos hechos en el tiempo. Despu\'es de analizar las deltas de tiempo hemos considerado razonable eliminar cualquier video que tarde m\'as de 40 d\'ias en convertirse en tendencia por dos razones; Si algo tarda m\'as de 40 d\'ias en convertirse en tendencia no proviene de v\'ideos de historias recientes. Los v\'ideos que tardan m\'as de 40 d\'as en convertirse en tendencia suponen menos del $1.5\%$ de la informaci\'on total. En la figura \ref{fig:delta} se presenta la m\'etodo de eliminaci\'on de estos datos.

\begin{figure}[h!]
\centering
\includegraphics[width=10cm]{tiempo.JPG}\\
\caption{Eliminamos las observaciones con una delta de tiempo superior a 40 d\'ias y se comprueba la cantidad de informaci\'on perdida}
\label{fig:delta}
\end{figure}

\section{An\'alisis de los datos}

\subsection{Selecci\'on de los grupos de datos que se quieren analizar/comparar (planificaci\'on de los análisis a aplicar).}

\subsubsection{V\'ideos con defectos}

De entre todas las columnas destacan unas con valores l\'ogicos:  {\itshape commentsdisabled}, {\itshape ratingsdisabled} y {\itshape video error or removed}, que est\'an marcados indicando alg\'un tipo de carencia. Con el fin de homogeneizar los datos, es decir, hacer que todos nuestros datos provengan de v\'ideos con las mismas caracter\'isticas generales, vamos a eliminar las filas con valores {\itshape True} en cualquiera de estos atributos. Esto se puede ver en la figura \ref{fig:defect}. Tambi\'en se puede observar que en general se conserva m\'as del $96\%$ de la informaci\'on. 


\begin{figure}[h!]
\centering
\includegraphics[width=13cm]{data_selection_2.JPG}
\caption{Eliminamos las v\'ideos con defectos y comprobamos cuanta informaci\'on se conserva}
\label{fig:defect}
\end{figure}




Debido a que estas columnas solo contienen valores {\itshape FALSE} despu\'es de elimar los {\itshape TRUE}, ya no es relevante conservar estas columnas por lo que se eliminan (esto se puede ver en c\'odigo en la secci\'on {\itshape data cleaning}).

\subsubsection{Eliminamos categor\'ia sin informaci\'on (category = 29)}

No existe informaci\'on sobre que categir\'ia representa el id 29, por lo tanto eliminamos \'esta del dataset. Otra raz\'on para eliminarla es que al no saber que representa dicha categor\'ia, aun en el caso que pudi\'esemos extraer alguna informaci\'on para la categor\'ia 29, no podr\'iamos darle un sentido concreto

En la figura \ref{fig:cat29}, se puede ver que despu\'es de eliminar los observaciones de esta categor\'ia, preservamos m\'as del 95 porciento de la informaci\'on.

\begin{figure}[h!]
\centering
\includegraphics[width=13cm]{remove_category.png}
\caption{Eliminamos la categor\'ia 29 y comprobamos el porcentaje de informci\'on conservada}
\label{fig:cat29}
\end{figure}





\subsubsection[item_metricas]{Generaci\'on de m\'etricas y an\'alisis de datos}

Nuestros datos en bruto pueden carecer de sentido sin una referencia; ?`Son dos v\'ideos con el mismo n\'umero de visitas igual de relevantes? o Para dos videos con el mismo n\'umero de likes, ?`Podemos pensar que han despertado el mismo inter\'es?. otro ejemplo ser\'ia tenemos videos con un gran n\'umero de visitas pero pocos comentarios (lo que llamariamos un click bait), o un video con pocas visitas pero un gran n\'umero de comentarios.

Para poder solventar inconveniente de que carecemos de referencias y poder analizar correctamente los v\'ideos, creamos una serie de m\'etricas relativas que nos ayudar\'an a realizar an\'alisis m\'as  de los v\'ideos.

\textbf{Sentimiento relativo:} \\
La m\'etrica {\itshape relative video sentiment} (rvs) se calcula utilizando la f\'ormula {\itshape (likes-dislikes)/sum(likes+dislikes)} y nos ayudar\'a a hacernos una idea de el sentimiento que genera un video sobre 1. Si el sentimiento que genera el v\'ideo es positivo el valor de rvs ser\'a positivos y si tiene m\'as {\itshape dislikes} el valor de rvs ser\'a negativo. Se puede ver expl\'icitamente el proceso seguido en la figura \ref{fig:rvsform}

\begin{figure}[h!]
\centering
\includegraphics[width=13cm]{rvs_gen.png}
\caption{Ecuaci\'on para generar rvs}
\label{fig:rvsform}
\end{figure}

Haciendo un plot de las frecuencias obtenidas sobre esta m\'etrica, nos damos cuenta de que los videos que llegan a ser {\itshape trending}, tienden a tener m\'as {\itshape likes} que {\itshape likes}. De hecho la mayar\'ia de v\'ideos que llegan a {\itshape ttrending} tienen al menos un $75\%$ de {\itshape likes} frente a {\itshape dislikes}. Esto se puede ver en la figura \ref{fig:rvsgraf}.

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{rvs_plot.png}
\caption{N\'umero de ocurrencias por rvs}
\label{fig:rvsgraf}
\end{figure}

\textbf{M\'etrica relevancia:}\\
Sabemos que para los usuarios comentar un video conlleva m\'as esfuerzo que hacer click a {\itshape like} o {\itshape dislike}. Tambi\'en entendemos que un video tiene mayor cantidad de comentarios cuando los viewers tienen algo que decir sobre el tema o alg\'un comentario a provocado controversia.

Para medir esto, hemos creado un atributo {\itshape rel{\textunderscore}elevance}, el cual se calcula con la formula {\itshape comment{\textunderscore}count/(likes+dislikes)}. Cuando su valor tiende a 1 asumimos que el video es relevante para los {\itshape viewers} y cuando es mayor que 1 quiere decir que es muy relevante. la geberaci\'on de la m\'etrica se puede ver la figura \ref{fig:relfor}.
\begin{figure}[h!]
\centering
\includegraphics[width=13cm]{rel_gen.png}
\caption{Ecuaci\'on para generar la m\'etrica de relevancia relativa}
\label{fig:relfor}

\end{figure}

Haciendo un plot de las frecuencias de este valor (figura \ref{fig:relfig}), podemos observar que la mayor parte de los videos tiene un valor menor que  1.  De hecho, observamos que los v\'ideos que alcanzan{\itshape trending} tienen a tener un ratio menor que 0.5, alcanzando un pico claro alrededor 0.25.

Esto nos da a entender que los v\'ideos que alcanzan {\itshape trending} no suelen ser muy pol\'emicos en general, lo cual tiene sentido ya que suelen ser v\'ideos que atraen a un p\'ublico general.

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{rel_relevance_plot.png}
\caption{N\'umero de ocurrencias para la relevancia relativa}
\label{fig:relfig}

\end{figure}

\textbf{M\'etrica sentiment engagement:}

Por \'ultimo vamos a generar 3 atributos relacionados con el{\itshape sentiment engagement} de los v\'ideos, estos ser\'an el {\itshape positive engagement}, {\itshape negative engagement} y {\itshape overall engagement}.
\newpage
Estas m\'etricas se calculan como: \footnote{Los c\'alculos se han realizado de la misma forma que se ha mostrado en la dem\'as m\'etricas. Los detalles pueden verse en el notebook, apratdo m\'etricas.}

\begin{itemize}
\item {\itshape Positive engagement} se calcular\'a utilizando la f\'ormula: {\itshape likes/views}


\item {\itshape Negative engagement} se calcular\'a utilizando la f\'ormula: {\itshape dislikes/views}



\item {\itshape Overall engagement} se calcular\'a utilizando la f\'ormula: {\itshape (likes-dislikes)/views}

\end{itemize}

Al representar sus frecuencias (figura \ref{fig:rengan}) podemos ver que los v\'ideos {\itshape trending} tienen un n\'umero de {\itshape likes} o {\itshape dislikes} muy peque\~no en comparaci\'on con las visualizaciones. De hecho la mayor\'ia  de ellos apenas tienen un $10\%$ de {\itshape likes} en comparaci\'on con las views. Y la mayor\'ia de las views son {\itshape likes}. Esto \'ultimo es m\'as evidente en la curva  para {\itshape Overall engagement} que para todos los casos, est\'a m\'as desplazada hacia el lado de los valores positivos.


\begin{figure}[h!]
\centering
\includegraphics[width=13cm]{engagement_1.png}
\includegraphics[width=13cm]{engagement_2.png}
\caption{N\'umero de ocurrencias para cada {\itshape sentiment engagement}}
\label{fig:rengan}
\end{figure}

\subsection{Comprobaci\'on de la normalidad y homogeneidad de la varianza.}

Nuestro daset presenta dos dificultades a la hora de comprobar si los datos y las m\'etricas siguen una distribuci\'on normal. Por una parte el gran n\'umero de datos hace inviable la aplicaci\'on del test de normalidad de {\itshape  Shapiro Wilk}, por otra parte, al existir muchos valores repetidos para un atributo el test de {\itshape Anderson Darling} no es fiable ya que siempre devolver\'a como resultado el rechazo de la hip\'otesis nula. Por este motivo nos centramos en los valores de {\itshape Kurosis} y {\itshape Skewness}. Para estos test, t\'ipicamnete se considera que una distribuci\'on es normal cuando el valor de {\itshape Kurtosis} est\'a entre $-2$ y $+2$ y para {\itshape skewness} los valores deben caer dentro del intervalo entre $-0.8$ y $+0.8$.

Hemos creado una funci\'on {\itshape normal check} que nos devuelve 1 cuando los datos de un atributo siguen una distribuci\'on normal (en las condiciones especificadas arriba) y 0 cuando no la sigue. La hemos aplicado sobre todas las columnas de todas los datasets y los datos los hemos visualizado como un dataframe que se copia abajo (los detalles de la funci\'on se encuentran el notebook).

Podemos comprobar que tanto los valores de los datos en crudo, como las m\'etricas que hemos implementado no siguen una distribuci\'on normal (figura \ref{fig:norm_1}). Con el fin de solucionar este problema y conseguir una distribuci\'on homogenea de la varianza, hemos optado por utilizar en primera instancia una transformaci\'on de {\itshape Yeo Jhonson}, puesto que nos permite aplicar el mismo c\'odigo sobre todos los atributos (una transformaci\'on de {\itshape Yeo Jhonson} a diferencia de {\itshape Boxcox} nos permite trabajar con valores negativos y ceros).




\begin{figure}
\centering
\includegraphics[width=12cm]{dataframe_1.JPG}
\includegraphics[width=12cm]{dataframe_2.JPG}
\caption{Comprobaci\'on de normalidad}
\label{fig:norm_1}
\end{figure}
 
Como se puede ver los datos en crudo se han podido transformar de forma que ahora siguen una distribuci\'on normal (figura \ref{fig:norm_1}). Los datos transformados se guardan en las columnas {\itshape fviews}, {\itshape likes}, {\itshape dislikes} y {\itshape fcommentl{\textunderscore}count}. Adem\'as los valores de lambda tambi\'en se guardan para poder deshacer la transformaci\'on (se pueden ver todos los detalles en el notebook en el apartado {\itshape Power transformation and descriptive statistics for the metrics}).

Sin embargo, las m\'etricas no siguen todav\'ia una distribuci\'on normal. Intentamos corregir este hecho hemos realizado una transformaci\'on por quantiles sobre las m\'etricas. Los nombres asociados a los datos de las m\'etricas trasnformados por quartiles son : {\itshape q{\textunderscore}rvs}, {\itshape q{\textunderscore}rel{\textunderscore}rel} y{\itshape q{\textunderscore}overall{\textunderscore}sen{\textunderscore}eng}. Una vez hechas las transformaciones, volvemos aplicar la funci\'on implementada para el c\'alculo de la normalidad de los atributos y representamos los resultados en un dataframe que se puede ver en la figura \ref{fig:quar}.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{dataframe_3.JPG}
\caption{Transformaci\'on por quartiles para las m\'etricas}
\label{fig:quar}
\end{figure}

Se ve una discrepancia en los datos de {\itshape M\'exico}. Hacemos un gr\'afico de densidades para la los datos {\itshape q\_overall\_sen\_eng}, es decir los datos que cuya variable  ha sufrido una transformaci\'on por quantiles.

\begin{figure}[h!]
\centering
\includegraphics[width=5cm]{grafica_transformada.JPG}
\caption{G\'afico de densidades para {\itshape q{\textunderscore}overall{\textunderscore}sen{\textunderscore}eng}}
\label{fig:quart_2}
\end{figure}

En la curva de la figura \ref{fig:quart_2}, no podemos apreciar ninguna diferencia significativa entre las curvas para cada pa\'is.

En conclusi\'on ahora contamos con los datos transformados  v\'ia {\itshape Yeo\-Jhonson} que siguen una distribuci\'on normal y la m\'etrica {\itshape overall\_sent\_eng} que transformada por quartiles tambi\'en sigue una distribuci\'on normal por lo que podremos aplicar test de correlaci\'on, modelos de regresi\'on y otros test estad\'isticos para m\'etricos.

Como nota final tambi\'en se ha investigado la normalidad de las deltas de tiempo, su distribuci\'on no es normal. Tampoco se ha podido alcanzar una distribuci\'on normal ni usando transformaciones exponenciales, ni usando transformaciones por quartiles (el c\'odigo con los c\'alculos se encuentran en el notebook)

\subsection[item_pruebas]{Aplicaci\'on de pruebas estad\'isticas para comparar los grupos de datos. En funci\'on de los datos y el objetivo del estudio, aplicar pruebas de contraste de hip\'otesis, correlaciones, regresiones, etc. Aplicar al menos tres m\'etodos de an\'alisis diferentes.}


\subsubsection{M\'etodo 1: Correlaci\'on}

\subsection{Cu\'ales son las 50 palabras m\'as populares por pa\'is y atributo}

Una de las formas que tenemos de seguir o analizar tendencias en videos de YouTube, es contar las palabras en los diferentes atributos de los videos.

En nuestro caso, vamos a contar las palabras en los titulos de los videos, comentarios, descripciones y canales.

Para empezar vamos a cargar los datos con los textos ya transformados y limpios.
\\
\\
\includegraphics[width=13cm]{50_words_1.png}
\\

Una vez tenemos los datos cargados, vamos a encontrar las 50 palabras mas populares para cada atributo del video.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{50_words_title.png}
\includegraphics[width=12cm]{50_words_channel.png}
\includegraphics[width=12cm]{50_words_tags.png}
\includegraphics[width=12cm]{50_words_desc.png}
\end{figure}

Por \'ultimo, vamos a realizar un wordcloud de los datos obtenidos.

Empezamos con los wordcloud de las tags.

Como podemos ver,  en alemania hay varias palabras en turco, esto se debe a la gran inmigraci\'on Turca que existe en el pais. Esto provoca que los datos est\'en divididos en turco y alem\'an.
\\
\\
El resto de wordclouds son bastante claros, a destacar en el Mejicano, que podemos ver la TV Azteca y exatlon, que es un programa de gran \'exito en el pais.
\\
\\
\includegraphics[width=13cm]{wordcloud_tags.png}
\\
\\
A continuaci\'on mostraremos el wordcloud de los t\'itulos de los videos.
\\
\\
A destacar que la palabra trump aparece m\'as en Alemania que en EEUU. Los videos musicales parecen ser muy populares. Los videos de f\'utbol y otros deportes son mucho mas populares en M\'ejico que en el resto de pa\'ises. Los vengadores y Star Wars tienen bastante impacto en UK y EEUU.
\\
\\
\includegraphics[width=13cm]{wordcloud_title.png}
\\
\\
Por \'ultimo vamos a mostrar las palabras mas utilizadas en las descripciones de los videos.
Como se puede observar, lo que mas se ven son nombres de redes sociales y webs, esto se debe a que la mayor\'ia de creadores de contenido introducen sus redes sociales en la descripci\'on. Para poder obtener informaci\'on relevante de \'esta wordcloud necesitamos realizar m\'as tareas de limpieza.
\\
\\
\includegraphics[width=13cm]{wordcould_desc.png}

\subsection{Cu\'anto tarda un video en llegar a Trending}
Para analizar cu\'anto tarda un video en llegar a Trending, vamos a generar una nueva columna con este valor, que calcularemos restando la fecha de Trending con la fecha de publicaci\'on.
\\
\\
\includegraphics[width=13cm]{time_delta_gen.png}
\\
\\
Una vez tenemos estos datos, vamos a realizar un plot de frecuencias para el n\'umero de videos por dias que tarda en llegar a trending.
\\
\\ 
\includegraphics[width=13cm]{plot_freq_times.png}
\\
\\
Con estas gr\'aficas podemos ver que la mayor\'ia de videos llegan a trending en los primeros d\'ias tras su lanzamiento. Para hacernos una mejor idea vamos a realizar un plot de que porcentajes de videos llegan a trending en diferentes rangos de tiempo.
\\
\\ 
\includegraphics[width=13cm]{plot_freq_code.png}
\\
\\ 
\includegraphics[width=13cm]{plot_freq_perc.png}
\\
\\
Los datos mostrados representan un 98\% de los videos. Estos resultados son muy interesantes ya que nos dan una idea de cuando tarda algo en hacerse popular en un los diferentes paises, cada cuanto utilizan la plataforma los usuarios en los diferentes paises, o cuanto tarda un usuario en ver algo fuera de sus intereses habituales.
\subsection{Qu\'e categor\'ias tienen m\'as \'exito en cada pa\'is}

Para realizar visualizaciones correctas sobre las categorias de los videos, vamos crear una columna con el nombre de la categor\'ia del video (ahora mismo solo teniamos sus ids).
\\
\\
\includegraphics[width=13cm]{categories_1.png}
\\
\\
Una vez tenemos la columna creada, vamos a obtener el numero total de videos en trending por categoria en cada mercado.
\\
\\
\includegraphics[width=13cm]{categories_2.png}
\\
\\
Con este plot vamos a ser capaces de ver la distribucion de las vistas por cada categoria. Como datos mas relevantes que se pueden observar, los videos de categor\'ia entretenimiento son los que suelen alcanzar trending con m\'as asiduidad.
\\
\\
Otro dato que me ha parecido muy interesante es como los videos musicales son muy populares en EEUU y UK, mientras que en Mejico o Alemania no son casi relevantes.\'Esta informaci\'on podr\'ia ser de gran utilidad para artistas que intentan crecer en diferentes regiones o mercados.
\\
\\
\includegraphics[width=13cm]{plot_categories.png}

\subsection{Modelo de Regresi\'on}
\'Esta secci\'on se ha realizado en R a diferencia de el resto de la pra\'actica que est\'a desarrollada en Python.



\section[item_conclusiones]{Conclusiones}
Hemos obtenido m\'ultiples conclusiones muy interesantes, hemos visto c\'omo los diferentes pa\'ises interact\'uan con los videos, ya sea desde el punto de vista de como reaccionan a ellos o cu\'ales son los m\'as populares.
\\
\\
Todas estas conclusiones ser\'an de gran utilidad para cualquier creador de contenido que trate de hacerse un hueco en YouTube, o incluso grandes empresas que quieran realizar propuestas de marketing a trav\'es de la plataforma podri\'ian utilizar \'esta informaci\'on para especializar los anuncios por comunidad o mercado.


\begin{thebibliography}{9}

\bibitem{td1} Leonard Berzkowitz,%
	\emph{ Advances in Experimental Social Psychology}, 1967,  Academic press
\bibitem{td2} Richard McElreath, Robert Boyd,%
	\emph{ Mathematical Models of Social Evolution: A Guide for the Perplexed}, 2007, The University of Chicago Press
\bibitem{td3}Thomas L. Saaty and Joyce M. Alexander,%
	\emph{Thinking with models: Mathematical Models in the Physical, Biological, and Social Sciences}, 2015, RWS Publications
\bibitem{dd1}Jason Radford and Kenneth Joseph,%
	\emph{Theory In, Theory Out: The Uses of Social Theory in Machine Learning for Social Science}, 2020, doi.org/10.3389/fdata.2020.00018

\end{thebibliography}

\end{document}
